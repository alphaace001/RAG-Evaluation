{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from trulens_eval import TruChain, Feedback, Tru\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from pinecone import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Tru initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67d0d43e8294910a85cbf3175241b2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d0014673284fecbd825ad0e5dca8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bd1dc7a98d4827829797f54d06a735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e1ab2a93a340968e8d365b75cce60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0289ef23a243d5b4c7bddb0b6e99e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9431b5cb1a9d4f5cb5229255f726083f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7352250b7b4aec85fdfd9d09f0d526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3c4b18bc7d24cebada6c4b320e99894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c640653c6fcc46458481daf5805aad42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08404fd3ddd3468da2fcae82c8ed8a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8812bf7db0ae4e1db3e63c688194adc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dffda69e5ec4d5183e7bb2226494319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model loaded\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the model\n",
    "Transformer_Name='all-distilroberta-v1'\n",
    "embeddings_model = SentenceTransformer(Transformer_Name)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=Transformer_Name)\n",
    "\n",
    "# Access the Hugging Face Transformer model\n",
    "hf_model = embeddings_model._first_module().auto_model\n",
    "\n",
    "# Get the word embedding dimension\n",
    "word_embedding_dimension = hf_model.config.hidden_size\n",
    "print(\"Embeddings model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"rag\"\n",
    "if index_name in pc.list_indexes():\n",
    "    print(f\"Deleting existing index: {index_name}\")\n",
    "    pc.delete_index(index_name)\n",
    "    # Wait for the index to be fully deleted\n",
    "    while index_name in pc.list_indexes():\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new index: rag\n",
      "Index rag is ready\n"
     ]
    }
   ],
   "source": [
    "# Create new Pinecone index\n",
    "print(f\"Creating new index: {index_name}\")\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=word_embedding_dimension,\n",
    "    metric='cosine',\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws',\n",
    "        region='us-east-1'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Index {index_name} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = \"D:/TruLens/attention.pdf\"\n",
    "raw_text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "documents = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored 41 document chunks in Pinecone index 'rag'\n"
     ]
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "for i, doc in enumerate(documents):\n",
    "    embedding = embeddings_model.encode(doc.page_content).tolist()\n",
    "    index.upsert(vectors=[(str(i), embedding, {\"text\": doc.page_content})])\n",
    "\n",
    "print(f\"Successfully stored {len(documents)} document chunks in Pinecone index '{index_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TruLens\\Trulensvenv\\Lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "vectorstore = LangchainPinecone(index, embeddings.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(\"Language model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    device=\"cpu\"  # Use \"cuda\" if you have a GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline setup complete\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"Pipeline setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA chain created\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"QA chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider.litellm import LiteLLM\n",
    "provider = LiteLLM(model_engine=\"ollama/llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Groundedness, input source will be set to __record__.app.retriever.invoke.rets[:].page_content.collect() .\n",
      "âœ… In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Q/A relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Q/A relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "âœ… In Q/Context relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Q/Context relevance, input context will be set to __record__.app.retriever.invoke.rets[:].page_content .\n"
     ]
    }
   ],
   "source": [
    "from trulens_eval.app import App\n",
    "context = App.select_context(qa_chain)\n",
    "\n",
    "# Define a groundedness feedback function\n",
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons,name=\"Groundedness\")\n",
    "    .on(context.collect()) # collect context chunks into a list\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "# Question/answer relevance between overall question and answer.\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance,name=\"Q/A relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "# Question/statement relevance between question and each context chunk.\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons,name=\"Q/Context relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = f\"Rag Using {Transformer_Name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the main topic of the document?\",\n",
    "    \"Who is the author of this document?\",\n",
    "    # Add more questions as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruChain created\n"
     ]
    }
   ],
   "source": [
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=[f_context_relevance,f_answer_relevance,f_groundedness]\n",
    ")\n",
    "print(\"TruChain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the main topic of the document?\n",
      "Answer: age recognition.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Record(record_id='record_hash_0d0c7815063d1f57861443459a01e732', app_id='Rag Using all-distilroberta-v1', cost=Cost(n_requests=0, n_successful_requests=0, n_classes=0, n_tokens=0, n_stream_chunks=0, n_prompt_tokens=0, n_completion_tokens=0, cost=0.0), perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 9, 511191), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 728650)), ts=datetime.datetime(2024, 7, 6, 19, 32, 17, 728650), tags='-', meta=None, main_input='What is the main topic of the document?', main_output='age recognition.', main_error=None, calls=[RecordAppCall(call_id='e1107a89-fdb0-4000-b63a-9b60a50be9af', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='get_relevant_documents')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='_get_relevant_documents'))], args={'query': 'What is the main topic of the document?', 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForRetrieverRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590107995536, 'init_bindings': None}}}, rets=[{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 10, 945096), end_time=datetime.datetime(2024, 7, 6, 19, 32, 11, 303673)), pid=19284, tid=2632), RecordAppCall(call_id='34bc725d-d649-4b9f-99b8-e71aa8dfa354', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='get_relevant_documents'))], args={'query': 'What is the main topic of the document?', 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164287056, 'init_bindings': None}}, 'tags': [], 'metadata': {}, 'run_name': None}, rets=[{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 10, 662160), end_time=datetime.datetime(2024, 7, 6, 19, 32, 11, 303673)), pid=19284, tid=2632), RecordAppCall(call_id='06be93b7-cd8c-48cf-a12a-96932e630857', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='invoke'))], args={'input': 'What is the main topic of the document?', 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164287056, 'init_bindings': None}}}}, rets=[{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 10, 374132), end_time=datetime.datetime(2024, 7, 6, 19, 32, 11, 303673)), pid=19284, tid=2632), RecordAppCall(call_id='423435b5-7318-476f-9b5f-2c9b8894d8ea', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='_call'))], args={'inputs': {'question': 'What is the main topic of the document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164411344, 'init_bindings': None}}}, rets={'text': 'age recognition.'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 13, 471077), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 710664)), pid=19284, tid=2632), RecordAppCall(call_id='2174c877-741b-4d5f-9c7c-5a0977838db0', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='invoke'))], args={'input': {'question': 'What is the main topic of the document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164086928, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'question': 'What is the main topic of the document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'text': 'age recognition.'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 13, 46848), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 712535)), pid=19284, tid=2632), RecordAppCall(call_id='d11dc807-1cae-429f-b4df-63f43ea13927', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='__call__'))], args={'inputs': {'question': 'What is the main topic of the document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164086928, 'init_bindings': None}}}, rets={'question': 'What is the main topic of the document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'text': 'age recognition.'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 12, 692034), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 713425)), pid=19284, tid=2632), RecordAppCall(call_id='29a672fa-d8f5-400e-be55-dfbaf0fcd23e', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call'))], args={'inputs': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164085712, 'init_bindings': None}}}, rets={'output_text': 'age recognition.'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 12, 377365), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 714395)), pid=19284, tid=2632), RecordAppCall(call_id='619c5c30-c10c-4dab-b8bc-f395b1d9a75d', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke'))], args={'input': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164079568, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'What is the main topic of the document?', 'output_text': 'age recognition.'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 12, 86295), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 714395)), pid=19284, tid=2632), RecordAppCall(call_id='72fa6555-d3f7-45da-9725-2bfb69b5f9b1', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__'))], args={'inputs': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164079568, 'init_bindings': None}}, 'tags': None, 'metadata': None}, rets={'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'What is the main topic of the document?', 'output_text': 'age recognition.'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 11, 806298), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 721557)), pid=19284, tid=2632), RecordAppCall(call_id='3dbffcbb-e398-419f-8d11-c076dc57782e', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run'))], args={'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164079568, 'init_bindings': None}}, 'kwargs': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}}, rets='age recognition.', error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 11, 536299), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 721557)), pid=19284, tid=2632), RecordAppCall(call_id='dd47e9e8-6700-4316-b414-dd42f95db12d', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call'))], args={'inputs': {'query': 'What is the main topic of the document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164286352, 'init_bindings': None}}}, rets={'result': 'age recognition.', 'source_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 10, 76779), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 726941)), pid=19284, tid=2632), RecordAppCall(call_id='515d59b2-9371-446b-80ab-2dc0fb0548a7', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke'))], args={'input': {'query': 'What is the main topic of the document?'}, 'config': {}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'query': 'What is the main topic of the document?', 'result': 'age recognition.', 'source_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 9, 806784), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 727649)), pid=19284, tid=2632), RecordAppCall(call_id='70013e90-ace9-4670-a8d9-442c902ab66b', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__'))], args={'inputs': {'query': 'What is the main topic of the document?'}}, rets={'query': 'What is the main topic of the document?', 'result': 'age recognition.', 'source_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'transduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n\\x03Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 32, 9, 511191), end_time=datetime.datetime(2024, 7, 6, 19, 32, 17, 728650)), pid=19284, tid=2632)], feedback_and_future_results=[(FeedbackDefinition(Q/Context relevance,\n",
       "\tselectors={'question': Lens().__record__.main_input, 'context': Lens().__record__.app.retriever.invoke.rets[:].page_content},\n",
       "\tif_exists=None\n",
       "), <Future at 0x25b0e77cb10 state=running>), (FeedbackDefinition(Q/A relevance,\n",
       "\tselectors={'prompt': Lens().__record__.main_input, 'response': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x25b0e893f90 state=running>), (FeedbackDefinition(Groundedness,\n",
       "\tselectors={'source': Lens().__record__.app.retriever.invoke.rets[:].page_content.collect(), 'statement': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x25b11d64910 state=running>)], feedback_results=[<Future at 0x25b0e77cb10 state=running>, <Future at 0x25b0e893f90 state=running>, <Future at 0x25b11d64910 state=running>])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedback Results:\n",
      "Q/Context relevance: 0.625\n",
      "Q/A relevance: 0.7\n",
      "Groundedness: -1.0\n",
      "\n",
      "Question: Who is the author of this document?\n",
      "Answer: [11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Record(record_id='record_hash_f61e45a8d0fe4c8bdf94277e7eeb4a78', app_id='Rag Using all-distilroberta-v1', cost=Cost(n_requests=0, n_successful_requests=0, n_classes=0, n_tokens=0, n_stream_chunks=0, n_prompt_tokens=0, n_completion_tokens=0, cost=0.0), perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 7, 899757), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 600708)), ts=datetime.datetime(2024, 7, 6, 19, 38, 15, 601708), tags='-', meta=None, main_input='Who is the author of this document?', main_output='[11]', main_error=None, calls=[RecordAppCall(call_id='ad3c138c-7976-4386-87e3-a5c96664c922', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='get_relevant_documents')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='_get_relevant_documents'))], args={'query': 'Who is the author of this document?', 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForRetrieverRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164473232, 'init_bindings': None}}}, rets=[{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 8, 982094), end_time=datetime.datetime(2024, 7, 6, 19, 38, 10, 304694)), pid=19284, tid=2632), RecordAppCall(call_id='8d8bc183-ce98-4b7d-9745-cceb129f167b', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='get_relevant_documents'))], args={'query': 'Who is the author of this document?', 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164274960, 'init_bindings': None}}, 'tags': [], 'metadata': {}, 'run_name': None}, rets=[{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 8, 767979), end_time=datetime.datetime(2024, 7, 6, 19, 38, 10, 305690)), pid=19284, tid=2632), RecordAppCall(call_id='f7d5f875-c6d2-4db0-b254-2d738289fa38', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2591932575632, init_bindings=None), name='invoke'))], args={'input': 'Who is the author of this document?', 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164274960, 'init_bindings': None}}}}, rets=[{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 8, 551627), end_time=datetime.datetime(2024, 7, 6, 19, 38, 10, 306693)), pid=19284, tid=2632), RecordAppCall(call_id='c938ab33-d519-4e16-bd32-71719c5fe1b3', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='_call'))], args={'inputs': {'question': 'Who is the author of this document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164374224, 'init_bindings': None}}}, rets={'text': '[11]'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 11, 820532), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 585672)), pid=19284, tid=2632), RecordAppCall(call_id='276ec935-1a15-42d9-8f5e-d8116764496e', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='invoke'))], args={'input': {'question': 'Who is the author of this document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590173305168, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'question': 'Who is the author of this document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'text': '[11]'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 11, 594156), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 587179)), pid=19284, tid=2632), RecordAppCall(call_id='bb5aaab3-fd52-4327-88ce-ee9a50ebba0e', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2590105212368, init_bindings=None), name='__call__'))], args={'inputs': {'question': 'Who is the author of this document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590173305168, 'init_bindings': None}}}, rets={'question': 'Who is the author of this document?', 'context': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\n\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'text': '[11]'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 11, 375053), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 588186)), pid=19284, tid=2632), RecordAppCall(call_id='a2195820-0675-4fa0-939d-390b308d8bc9', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='_call'))], args={'inputs': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590173300560, 'init_bindings': None}}}, rets={'output_text': '[11]'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 11, 158542), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 590191)), pid=19284, tid=2632), RecordAppCall(call_id='433bea74-e5b3-441e-8552-9a9f991020a3', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='invoke'))], args={'input': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164277136, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'Who is the author of this document?', 'output_text': '[11]'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 10, 940212), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 592191)), pid=19284, tid=2632), RecordAppCall(call_id='dc04254e-526a-49ac-bf0b-ee3a0b24a1b6', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='__call__'))], args={'inputs': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164277136, 'init_bindings': None}}, 'tags': None, 'metadata': None}, rets={'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'Who is the author of this document?', 'output_text': '[11]'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 10, 717089), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 594192)), pid=19284, tid=2632), RecordAppCall(call_id='5e7dd44e-8c9b-4808-99b4-8e2cc4a92989', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2590173421520, init_bindings=None), name='run'))], args={'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164277136, 'init_bindings': None}}, 'kwargs': {'input_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}}, rets='[11]', error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 10, 485192), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 597703)), pid=19284, tid=2632), RecordAppCall(call_id='c7292b19-3689-4228-98e5-47cedc748c9c', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='_call'))], args={'inputs': {'query': 'Who is the author of this document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2590164286032, 'init_bindings': None}}}, rets={'result': '[11]', 'source_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 8, 342648), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 598708)), pid=19284, tid=2632), RecordAppCall(call_id='5a5e9dd4-e26d-4dfc-acf7-60340aa974de', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='invoke'))], args={'input': {'query': 'Who is the author of this document?'}, 'config': {}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'query': 'Who is the author of this document?', 'result': '[11]', 'source_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 8, 129923), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 599708)), pid=19284, tid=2632), RecordAppCall(call_id='cebecdfc-6883-495f-85aa-ec628faac93a', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2590121368592, init_bindings=None), name='__call__'))], args={'inputs': {'query': 'Who is the author of this document?'}}, rets={'query': 'Who is the author of this document?', 'result': '[11]', 'source_documents': [{'page_content': 'age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and JÃ¼rgen Schmidhuber. Gradient ï¬‚ow in\\nrecurrent nets: the difï¬culty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735â€“1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Åukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overï¬tting. Journal of Machine\\nLearning Research , 15(1):1929â€“1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440â€“2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104â€“3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.', 'metadata': {}, 'type': 'Document'}, {'page_content': 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Ã‡aglar GÃ¼lÃ§ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770â€“778, 2016.', 'metadata': {}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 19, 38, 7, 899757), end_time=datetime.datetime(2024, 7, 6, 19, 38, 15, 600708)), pid=19284, tid=2632)], feedback_and_future_results=[(FeedbackDefinition(Q/Context relevance,\n",
       "\tselectors={'question': Lens().__record__.main_input, 'context': Lens().__record__.app.retriever.invoke.rets[:].page_content},\n",
       "\tif_exists=None\n",
       "), <Future at 0x25b002c5ed0 state=running>), (FeedbackDefinition(Q/A relevance,\n",
       "\tselectors={'prompt': Lens().__record__.main_input, 'response': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x25b125d2950 state=running>), (FeedbackDefinition(Groundedness,\n",
       "\tselectors={'source': Lens().__record__.app.retriever.invoke.rets[:].page_content.collect(), 'statement': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x25b11d50590 state=pending>)], feedback_results=[<Future at 0x25b002c5ed0 state=running>, <Future at 0x25b125d2950 state=running>, <Future at 0x25b11d50590 state=pending>])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedback Results:\n",
      "Q/Context relevance: 0.625\n",
      "Q/A relevance: 1.0\n",
      "Groundedness: -1.0\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    with tru_recorder as recording:\n",
    "        response = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {response['result']}\")\n",
    "    \n",
    "    # Retrieve the record of the app invocation\n",
    "    rec = recording.get()  \n",
    "    \n",
    "    # Display the record\n",
    "    display(rec)\n",
    "    \n",
    "    # Wait for feedback results and print them\n",
    "    print(\"\\nFeedback Results:\")\n",
    "    for feedback, feedback_result in rec.wait_for_feedback_results().items():\n",
    "        print(f\"{feedback.name}: {feedback_result.result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.to_numpy of                            app_id  \\\n",
       "0  Rag Using all-distilroberta-v1   \n",
       "1  Rag Using all-distilroberta-v1   \n",
       "\n",
       "                                            app_json  \\\n",
       "0  {\"tru_class_info\": {\"name\": \"TruChain\", \"modul...   \n",
       "1  {\"tru_class_info\": {\"name\": \"TruChain\", \"modul...   \n",
       "\n",
       "                                              type  \\\n",
       "0  RetrievalQA(langchain.chains.retrieval_qa.base)   \n",
       "1  RetrievalQA(langchain.chains.retrieval_qa.base)   \n",
       "\n",
       "                                      record_id  \\\n",
       "0  record_hash_0d0c7815063d1f57861443459a01e732   \n",
       "1  record_hash_f61e45a8d0fe4c8bdf94277e7eeb4a78   \n",
       "\n",
       "                                       input              output tags  \\\n",
       "0  \"What is the main topic of the document?\"  \"age recognition.\"    -   \n",
       "1      \"Who is the author of this document?\"              \"[11]\"    -   \n",
       "\n",
       "                                         record_json  \\\n",
       "0  {\"record_id\": \"record_hash_0d0c7815063d1f57861...   \n",
       "1  {\"record_id\": \"record_hash_f61e45a8d0fe4c8bdf9...   \n",
       "\n",
       "                                           cost_json  \\\n",
       "0  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "1  {\"n_requests\": 0, \"n_successful_requests\": 0, ...   \n",
       "\n",
       "                                           perf_json  \\\n",
       "0  {\"start_time\": \"2024-07-06T19:32:09.511191\", \"...   \n",
       "1  {\"start_time\": \"2024-07-06T19:38:07.899757\", \"...   \n",
       "\n",
       "                           ts  Groundedness  Q/A relevance  \\\n",
       "0  2024-07-06T19:32:17.728650          -1.0            0.7   \n",
       "1  2024-07-06T19:38:15.601708          -1.0            1.0   \n",
       "\n",
       "   Q/Context relevance                                 Groundedness_calls  \\\n",
       "0                0.625  [{'args': {'source': ['age recognition. In Pro...   \n",
       "1                0.625  [{'args': {'source': ['age recognition. In Pro...   \n",
       "\n",
       "                                 Q/A relevance_calls  \\\n",
       "0  [{'args': {'prompt': 'What is the main topic o...   \n",
       "1  [{'args': {'prompt': 'Who is the author of thi...   \n",
       "\n",
       "                           Q/Context relevance_calls  latency  total_tokens  \\\n",
       "0  [{'args': {'question': 'What is the main topic...        8             0   \n",
       "1  [{'args': {'question': 'Who is the author of t...        7             0   \n",
       "\n",
       "   total_cost  \n",
       "0         0.0  \n",
       "1         0.0  >"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "records.to_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n",
      "Dashboard already running at path:   Network URL: http://192.168.1.5:8501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully deleted Pinecone index: rag\n"
     ]
    }
   ],
   "source": [
    "def delete_pinecone_index(index_name):\n",
    "    try:\n",
    "        if index_name in pc.list_indexes().names():\n",
    "            pc.delete_index(index_name)\n",
    "            print(f\"Successfully deleted Pinecone index: {index_name}\")\n",
    "        else:\n",
    "            print(f\"Index {index_name} does not exist or has already been deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting Pinecone index: {str(e)}\")\n",
    "delete_pinecone_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trulensvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
