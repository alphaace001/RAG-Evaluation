{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader  # Changed from TextLoader to PyPDFLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from trulens_eval import TruChain, Feedback, Tru\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦑 Tru initialized with db url sqlite:///default.sqlite .\n",
      "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `Tru` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "tru = Tru()\n",
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 pages from the PDF\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"D:/TruLens/attention.pdf\"  # Change this to the path of your PDF file\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages from the PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 11 text chunks\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "print(f\"Split into {len(texts)} text chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings model loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")  # 768 dimensions\n",
    "print(\"Embeddings model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "print(\"Vector store created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(\"Language model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512,\n",
    "    device=\"cpu\"  # Use \"cuda\" if you have a GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline setup complete\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "print(\"Pipeline setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA chain created\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "print(\"QA chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider.litellm import LiteLLM\n",
    "provider = LiteLLM(model_engine=\"ollama/llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.app import App\n",
    "context = App.select_context(qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Groundedness, input source will be set to __record__.app.retriever.invoke.rets[:].page_content.collect() .\n",
      "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Q/A relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Q/A relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Q/Context relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Q/Context relevance, input context will be set to __record__.app.retriever.invoke.rets[:].page_content .\n"
     ]
    }
   ],
   "source": [
    "f_groundedness = (\n",
    "    Feedback(provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(context.collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "f_answer_relevance = (\n",
    "    Feedback(provider.relevance, name=\"Q/A relevance\")\n",
    "    .on_input_output()\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance_with_cot_reasons, name=\"Q/Context relevance\")\n",
    "    .on_input()\n",
    "    .on(context)\n",
    "    .aggregate(np.mean)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruChain created\n"
     ]
    }
   ],
   "source": [
    "app_id = \"RAG_QA_Chain_PDF\"\n",
    "tru_recorder = TruChain(\n",
    "    qa_chain,\n",
    "    app_id=app_id,\n",
    "    feedbacks=[f_context_relevance, f_answer_relevance, f_groundedness]\n",
    ")\n",
    "print(\"TruChain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the main topic of the document?\",\n",
    "    \"Who is the author of this document?\",\n",
    "    # Add more questions as needed\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the main topic of the document?\n",
      "Answer: Attention-based neural machine translation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Record(record_id='record_hash_10f3a380a2cc7581f2e1813d4caaa91b', app_id='RAG_QA_Chain_PDF', cost=Cost(n_requests=0, n_successful_requests=0, n_classes=0, n_tokens=0, n_stream_chunks=0, n_prompt_tokens=0, n_completion_tokens=0, cost=0.0), perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 30, 442769), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 460004)), ts=datetime.datetime(2024, 7, 6, 0, 24, 57, 462020), tags='-', meta=None, main_input='What is the main topic of the document?', main_output='Attention-based neural machine translation', main_error=None, calls=[RecordAppCall(call_id='91d0a916-6ee1-432b-90d8-d34a49bfd19b', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='get_relevant_documents')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='_get_relevant_documents'))], args={'query': 'What is the main topic of the document?', 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForRetrieverRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550940688, 'init_bindings': None}}}, rets=[{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 32, 430577), end_time=datetime.datetime(2024, 7, 6, 0, 24, 32, 561161)), pid=2676, tid=3716), RecordAppCall(call_id='4ca51dfd-0acb-455d-8fe0-2983f2fe4eff', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='get_relevant_documents'))], args={'query': 'What is the main topic of the document?', 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550620944, 'init_bindings': None}}, 'tags': [], 'metadata': {}, 'run_name': None}, rets=[{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 32, 57711), end_time=datetime.datetime(2024, 7, 6, 0, 24, 32, 563166)), pid=2676, tid=3716), RecordAppCall(call_id='3d8f862d-49d5-4b87-a921-b3ece843e26e', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='invoke'))], args={'input': 'What is the main topic of the document?', 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550620944, 'init_bindings': None}}}}, rets=[{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 31, 689671), end_time=datetime.datetime(2024, 7, 6, 0, 24, 32, 564170)), pid=2676, tid=3716), RecordAppCall(call_id='0f8a8ad1-a800-4554-9863-e1bbc91f9793', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='_call'))], args={'inputs': {'question': 'What is the main topic of the document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6\\n\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836549236752, 'init_bindings': None}}}, rets={'text': 'Attention-based neural machine translation'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 35, 243936), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 443975)), pid=2676, tid=3716), RecordAppCall(call_id='d934344e-3428-4e8e-976b-e634e348273f', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='invoke'))], args={'input': {'question': 'What is the main topic of the document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6\\n\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836606240592, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'question': 'What is the main topic of the document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6\\n\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'text': 'Attention-based neural machine translation'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 34, 850224), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 445980)), pid=2676, tid=3716), RecordAppCall(call_id='21ea2b80-66b8-44d1-a5eb-31dc5aa0a533', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='__call__'))], args={'inputs': {'question': 'What is the main topic of the document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6\\n\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836606240592, 'init_bindings': None}}}, rets={'question': 'What is the main topic of the document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6\\n\\nReferences\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'text': 'Attention-based neural machine translation'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 34, 453136), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 445980)), pid=2676, tid=3716), RecordAppCall(call_id='a6c65702-ad04-4e18-a715-59c65b0c38ed', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call'))], args={'inputs': {'input_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836606244432, 'init_bindings': None}}}, rets={'output_text': 'Attention-based neural machine translation'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 34, 69854), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 447975)), pid=2676, tid=3716), RecordAppCall(call_id='446f8505-de02-4a47-8cee-7e3cc3c89464', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke'))], args={'input': {'input_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550653264, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'input_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'What is the main topic of the document?', 'output_text': 'Attention-based neural machine translation'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 33, 696208), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 450489)), pid=2676, tid=3716), RecordAppCall(call_id='b6adb51c-b3bc-463a-bed9-edfcad2ccb9e', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__'))], args={'inputs': {'input_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550653264, 'init_bindings': None}}, 'tags': None, 'metadata': None}, rets={'input_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'What is the main topic of the document?', 'output_text': 'Attention-based neural machine translation'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 33, 325904), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 453499)), pid=2676, tid=3716), RecordAppCall(call_id='ebb5e318-fef4-4cf0-8206-a8232a61daef', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run'))], args={'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550653264, 'init_bindings': None}}, 'kwargs': {'input_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'What is the main topic of the document?'}}, rets='Attention-based neural machine translation', error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 32, 930556), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 456498)), pid=2676, tid=3716), RecordAppCall(call_id='44db583c-9eca-4fec-9e20-b68ac9baacc1', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call'))], args={'inputs': {'query': 'What is the main topic of the document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836453472144, 'init_bindings': None}}}, rets={'result': 'Attention-based neural machine translation', 'source_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 31, 326848), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 457505)), pid=2676, tid=3716), RecordAppCall(call_id='bf98d7d1-a574-4a82-b507-529e03cefd50', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke'))], args={'input': {'query': 'What is the main topic of the document?'}, 'config': {}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'query': 'What is the main topic of the document?', 'result': 'Attention-based neural machine translation', 'source_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 30, 968685), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 458498)), pid=2676, tid=3716), RecordAppCall(call_id='3b5cbc84-e20c-45a7-986e-e409a3a0ceb9', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__'))], args={'inputs': {'query': 'What is the main topic of the document?'}}, rets={'query': 'What is the main topic of the document?', 'result': 'Attention-based neural machine translation', 'source_documents': [{'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'metadata': {'page': 5, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 24, 30, 442769), end_time=datetime.datetime(2024, 7, 6, 0, 24, 57, 460004)), pid=2676, tid=3716)], feedback_and_future_results=[(FeedbackDefinition(Q/Context relevance,\n",
       "\tselectors={'question': Lens().__record__.main_input, 'context': Lens().__record__.app.retriever.invoke.rets[:].page_content},\n",
       "\tif_exists=None\n",
       "), <Future at 0x29472f05650 state=running>), (FeedbackDefinition(Q/A relevance,\n",
       "\tselectors={'prompt': Lens().__record__.main_input, 'response': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x29472f101d0 state=running>), (FeedbackDefinition(Groundedness,\n",
       "\tselectors={'source': Lens().__record__.app.retriever.invoke.rets[:].page_content.collect(), 'statement': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x2946d598c50 state=running>)], feedback_results=[<Future at 0x29472f05650 state=running>, <Future at 0x29472f101d0 state=running>, <Future at 0x2946d598c50 state=running>])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedback Results:\n",
      "Q/Context relevance: 0.9\n",
      "Q/A relevance: 0.8\n",
      "Groundedness: 0.9\n",
      "\n",
      "Question: Who is the author of this document?\n",
      "Answer: Nal Kalchbrenner and Stephan Gouws\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Record(record_id='record_hash_50f4a5ce85a1092a30a6f84f893569bb', app_id='RAG_QA_Chain_PDF', cost=Cost(n_requests=0, n_successful_requests=0, n_classes=0, n_tokens=0, n_stream_chunks=0, n_prompt_tokens=0, n_completion_tokens=0, cost=0.0), perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 46, 371196), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 161489)), ts=datetime.datetime(2024, 7, 6, 0, 34, 11, 162487), tags='-', meta=None, main_input='Who is the author of this document?', main_output='Nal Kalchbrenner and Stephan Gouws', main_error=None, calls=[RecordAppCall(call_id='2fa4a4b5-c7fe-4ca1-bf81-63511e4f7f02', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='get_relevant_documents')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='_get_relevant_documents'))], args={'query': 'Who is the author of this document?', 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForRetrieverRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836669458960, 'init_bindings': None}}}, rets=[{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 47, 943327), end_time=datetime.datetime(2024, 7, 6, 0, 33, 48, 111945)), pid=2676, tid=3716), RecordAppCall(call_id='0116609b-62d5-4685-b6e5-b7671264cc9c', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='get_relevant_documents'))], args={'query': 'Who is the author of this document?', 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550662416, 'init_bindings': None}}, 'tags': [], 'metadata': {}, 'run_name': None}, rets=[{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 47, 626399), end_time=datetime.datetime(2024, 7, 6, 0, 33, 48, 113947)), pid=2676, tid=3716), RecordAppCall(call_id='65382e55-3d9c-4c97-84d7-8b64493466fb', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.retriever, method=Method(obj=Obj(cls=langchain_core.vectorstores.VectorStoreRetriever, id=2836485786704, init_bindings=None), name='invoke'))], args={'input': 'Who is the author of this document?', 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550662416, 'init_bindings': None}}}}, rets=[{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 47, 307602), end_time=datetime.datetime(2024, 7, 6, 0, 33, 48, 114948)), pid=2676, tid=3716), RecordAppCall(call_id='eba0f63d-1390-4c1f-8824-f32a93c203cc', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='_call'))], args={'inputs': {'question': 'Who is the author of this document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836669458896, 'init_bindings': None}}}, rets={'text': 'Nal Kalchbrenner and Stephan Gouws'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 50, 808967), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 144931)), pid=2676, tid=3716), RecordAppCall(call_id='6eb7af55-70cb-4ae0-80df-806798d5b420', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='invoke'))], args={'input': {'question': 'Who is the author of this document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836606814736, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'question': 'Who is the author of this document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'text': 'Nal Kalchbrenner and Stephan Gouws'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 50, 474845), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 146931)), pid=2676, tid=3716), RecordAppCall(call_id='94208556-9570-4df9-acb0-a7eba411a900', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain.llm_chain, method=Method(obj=Obj(cls=langchain.chains.llm.LLMChain, id=2836485791376, init_bindings=None), name='__call__'))], args={'inputs': {'question': 'Who is the author of this document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836606814736, 'init_bindings': None}}}, rets={'question': 'Who is the author of this document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11\\n\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'text': 'Nal Kalchbrenner and Stephan Gouws'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 50, 90571), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 148020)), pid=2676, tid=3716), RecordAppCall(call_id='f35ec913-06ab-4151-be3d-8aa4573e6bfc', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='_call'))], args={'inputs': {'input_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836606821968, 'init_bindings': None}}}, rets={'output_text': 'Nal Kalchbrenner and Stephan Gouws'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 49, 645794), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 149972)), pid=2676, tid=3716), RecordAppCall(call_id='6024ecf6-96f4-4ab3-bf5a-9c5cebaac0e0', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='invoke'))], args={'input': {'input_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}, 'config': {'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550662416, 'init_bindings': None}}}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'input_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'Who is the author of this document?', 'output_text': 'Nal Kalchbrenner and Stephan Gouws'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 49, 194258), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 151972)), pid=2676, tid=3716), RecordAppCall(call_id='916c6911-1768-40c7-99fc-065917e37d0c', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='__call__'))], args={'inputs': {'input_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}, 'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550662416, 'init_bindings': None}}, 'tags': None, 'metadata': None}, rets={'input_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'Who is the author of this document?', 'output_text': 'Nal Kalchbrenner and Stephan Gouws'}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 48, 802460), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 154969)), pid=2676, tid=3716), RecordAppCall(call_id='3dc47e82-51b4-4fa4-b549-3e14303a452f', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call')), RecordAppCallMethod(path=Lens().app.combine_documents_chain, method=Method(obj=Obj(cls=langchain.chains.combine_documents.stuff.StuffDocumentsChain, id=2836523446480, init_bindings=None), name='run'))], args={'callbacks': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManager', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836550662416, 'init_bindings': None}}, 'kwargs': {'input_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}], 'question': 'Who is the author of this document?'}}, rets='Nal Kalchbrenner and Stephan Gouws', error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 48, 460350), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 158480)), pid=2676, tid=3716), RecordAppCall(call_id='fae79f7b-58f0-4d9e-b190-d24311b0746a', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='_call'))], args={'inputs': {'query': 'Who is the author of this document?'}, 'run_manager': {'__tru_non_serialized_object': {'cls': {'name': 'CallbackManagerForChainRun', 'module': {'package_name': 'langchain_core.callbacks', 'module_name': 'langchain_core.callbacks.manager'}, 'bases': None}, 'id': 2836669448464, 'init_bindings': None}}}, rets={'result': 'Nal Kalchbrenner and Stephan Gouws', 'source_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 46, 997697), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 158480)), pid=2676, tid=3716), RecordAppCall(call_id='25d9e38f-42d7-4c85-b742-fee843c322c1', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__')), RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='invoke'))], args={'input': {'query': 'Who is the author of this document?'}, 'config': {}, 'kwargs': {'return_only_outputs': False, 'include_run_info': False}}, rets={'query': 'Who is the author of this document?', 'result': 'Nal Kalchbrenner and Stephan Gouws', 'source_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 46, 693422), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 160487)), pid=2676, tid=3716), RecordAppCall(call_id='d047ebc5-e049-4751-a26f-9580e68cdf74', stack=[RecordAppCallMethod(path=Lens().app, method=Method(obj=Obj(cls=langchain.chains.retrieval_qa.base.RetrievalQA, id=2836511782864, init_bindings=None), name='__call__'))], args={'inputs': {'query': 'Who is the author of this document?'}}, rets={'query': 'Who is the author of this document?', 'result': 'Nal Kalchbrenner and Stephan Gouws', 'source_documents': [{'page_content': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'metadata': {'page': 9, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'metadata': {'page': 10, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'metadata': {'page': 0, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}, {'page_content': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9', 'metadata': {'page': 8, 'source': 'D:/TruLens/attention.pdf'}, 'type': 'Document'}]}, error=None, perf=Perf(start_time=datetime.datetime(2024, 7, 6, 0, 33, 46, 371196), end_time=datetime.datetime(2024, 7, 6, 0, 34, 11, 161489)), pid=2676, tid=3716)], feedback_and_future_results=[(FeedbackDefinition(Q/Context relevance,\n",
       "\tselectors={'question': Lens().__record__.main_input, 'context': Lens().__record__.app.retriever.invoke.rets[:].page_content},\n",
       "\tif_exists=None\n",
       "), <Future at 0x29476d7af10 state=running>), (FeedbackDefinition(Q/A relevance,\n",
       "\tselectors={'prompt': Lens().__record__.main_input, 'response': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x29476de4090 state=running>), (FeedbackDefinition(Groundedness,\n",
       "\tselectors={'source': Lens().__record__.app.retriever.invoke.rets[:].page_content.collect(), 'statement': Lens().__record__.main_output},\n",
       "\tif_exists=None\n",
       "), <Future at 0x29476e0a690 state=pending>)], feedback_results=[<Future at 0x29476d7af10 state=running>, <Future at 0x29476de4090 state=running>, <Future at 0x29476e0a690 state=pending>])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feedback Results:\n",
      "Q/Context relevance: 0.9\n",
      "Q/A relevance: 0.8\n",
      "Groundedness: 0.7\n"
     ]
    }
   ],
   "source": [
    "for question in questions:\n",
    "    with tru_recorder as recording:\n",
    "        response = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: {response['result']}\")\n",
    "    \n",
    "    # Retrieve the record of the app invocation\n",
    "    rec = recording.get()  \n",
    "    \n",
    "    # Display the record\n",
    "    display(rec)\n",
    "    \n",
    "    # Wait for feedback results and print them\n",
    "    print(\"\\nFeedback Results:\")\n",
    "    for feedback, feedback_result in rec.wait_for_feedback_results().items():\n",
    "        print(f\"{feedback.name}: {feedback_result.result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[app_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['RAG_QA_Chain_PDF',\n",
       "        '{\"tru_class_info\": {\"name\": \"TruChain\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.tru_chain\"}, \"bases\": [{\"name\": \"TruChain\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.tru_chain\"}, \"bases\": null}, {\"name\": \"App\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.app\"}, \"bases\": null}, {\"name\": \"AppDefinition\", \"module\": {\"package_name\": \"trulens_eval.schema\", \"module_name\": \"trulens_eval.schema.app\"}, \"bases\": null}, {\"name\": \"WithClassInfo\", \"module\": {\"package_name\": \"trulens_eval.utils\", \"module_name\": \"trulens_eval.utils.pyschema\"}, \"bases\": null}, {\"name\": \"SerialModel\", \"module\": {\"package_name\": \"trulens_eval.utils\", \"module_name\": \"trulens_eval.utils.serial\"}, \"bases\": null}, {\"name\": \"BaseModel\", \"module\": {\"package_name\": \"pydantic\", \"module_name\": \"pydantic.main\"}, \"bases\": null}, {\"name\": \"WithInstrumentCallbacks\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.instruments\"}, \"bases\": null}, {\"name\": \"Hashable\", \"module\": {\"package_name\": \"collections\", \"module_name\": \"collections.abc\"}, \"bases\": null}, {\"name\": \"Generic\", \"module\": {\"package_name\": \"\", \"module_name\": \"typing\"}, \"bases\": null}, {\"name\": \"object\", \"module\": {\"package_name\": \"\", \"module_name\": \"builtins\"}, \"bases\": null}]}, \"app_id\": \"RAG_QA_Chain_PDF\", \"tags\": \"-\", \"metadata\": {}, \"feedback_definitions\": [], \"feedback_mode\": \"with_app_thread\", \"root_class\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"app\": {\"name\": null, \"memory\": null, \"callbacks\": null, \"verbose\": false, \"tags\": null, \"metadata\": null, \"callback_manager\": null, \"combine_documents_chain\": {\"name\": null, \"memory\": null, \"callbacks\": null, \"verbose\": false, \"tags\": null, \"metadata\": null, \"callback_manager\": null, \"input_key\": \"input_documents\", \"output_key\": \"output_text\", \"llm_chain\": {\"name\": null, \"memory\": null, \"callbacks\": null, \"verbose\": false, \"tags\": null, \"metadata\": null, \"callback_manager\": null, \"prompt\": {\"name\": null, \"input_variables\": [\"context\", \"question\"], \"input_types\": {}, \"output_parser\": null, \"partial_variables\": {}, \"metadata\": null, \"tags\": null, \"template\": \"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\\\n\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", \"template_format\": \"f-string\", \"validate_template\": false}, \"llm\": {\"name\": null, \"cache\": null, \"verbose\": false, \"callbacks\": null, \"tags\": null, \"metadata\": null, \"custom_get_token_ids\": null, \"callback_manager\": null, \"pipeline\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"Text2TextGenerationPipeline\", \"module\": {\"package_name\": \"transformers.pipelines\", \"module_name\": \"transformers.pipelines.text2text_generation\"}, \"bases\": null}, \"id\": 2836523448528, \"init_bindings\": null}}, \"model_id\": \"gpt2\", \"model_kwargs\": null, \"pipeline_kwargs\": null, \"batch_size\": 4}, \"output_key\": \"text\", \"output_parser\": {\"name\": null}, \"return_final_only\": true, \"llm_kwargs\": {}}, \"document_prompt\": {\"name\": null, \"input_variables\": [\"page_content\"], \"input_types\": {}, \"output_parser\": null, \"partial_variables\": {}, \"metadata\": null, \"tags\": null, \"template\": \"{page_content}\", \"template_format\": \"f-string\", \"validate_template\": false}, \"document_variable_name\": \"context\", \"document_separator\": \"\\\\n\\\\n\"}, \"input_key\": \"query\", \"output_key\": \"result\", \"return_source_documents\": true, \"retriever\": {\"name\": null, \"tags\": [\"Chroma\", \"HuggingFaceEmbeddings\"], \"metadata\": null, \"vectorstore\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"Chroma\", \"module\": {\"package_name\": \"langchain_community.vectorstores\", \"module_name\": \"langchain_community.vectorstores.chroma\"}, \"bases\": null}, \"id\": 2836491291280, \"init_bindings\": null}}, \"search_type\": \"similarity\", \"search_kwargs\": {}}}, \"initial_app_loader_dump\": null, \"app_extra_json\": {}, \"selector_check_warning\": false, \"selector_nocheck\": false}',\n",
       "        'RetrievalQA(langchain.chains.retrieval_qa.base)',\n",
       "        'record_hash_10f3a380a2cc7581f2e1813d4caaa91b',\n",
       "        '\"What is the main topic of the document?\"',\n",
       "        '\"Attention-based neural machine translation\"', '-',\n",
       "        '{\"record_id\": \"record_hash_10f3a380a2cc7581f2e1813d4caaa91b\", \"app_id\": \"RAG_QA_Chain_PDF\", \"cost\": {\"n_requests\": 0, \"n_successful_requests\": 0, \"n_classes\": 0, \"n_tokens\": 0, \"n_stream_chunks\": 0, \"n_prompt_tokens\": 0, \"n_completion_tokens\": 0, \"cost\": 0.0}, \"perf\": {\"start_time\": \"2024-07-06T00:24:30.442769\", \"end_time\": \"2024-07-06T00:24:57.460004\"}, \"ts\": \"2024-07-06T00:24:57.462020\", \"tags\": \"-\", \"meta\": null, \"main_input\": \"What is the main topic of the document?\", \"main_output\": \"Attention-based neural machine translation\", \"main_error\": null, \"calls\": [{\"call_id\": \"91d0a916-6ee1-432b-90d8-d34a49bfd19b\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"get_relevant_documents\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"_get_relevant_documents\"}}], \"args\": {\"query\": \"What is the main topic of the document?\", \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForRetrieverRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550940688, \"init_bindings\": null}}}, \"rets\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:32.430577\", \"end_time\": \"2024-07-06T00:24:32.561161\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"4ca51dfd-0acb-455d-8fe0-2983f2fe4eff\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"get_relevant_documents\"}}], \"args\": {\"query\": \"What is the main topic of the document?\", \"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550620944, \"init_bindings\": null}}, \"tags\": [], \"metadata\": {}, \"run_name\": null}, \"rets\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:32.057711\", \"end_time\": \"2024-07-06T00:24:32.563166\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"3d8f862d-49d5-4b87-a921-b3ece843e26e\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": \"What is the main topic of the document?\", \"config\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550620944, \"init_bindings\": null}}}}, \"rets\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:31.689671\", \"end_time\": \"2024-07-06T00:24:32.564170\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"0f8a8ad1-a800-4554-9863-e1bbc91f9793\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"_call\"}}], \"args\": {\"inputs\": {\"question\": \"What is the main topic of the document?\", \"context\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\\\\n\\\\nReferences\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\"}, \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForChainRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836549236752, \"init_bindings\": null}}}, \"rets\": {\"text\": \"Attention-based neural machine translation\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:35.243936\", \"end_time\": \"2024-07-06T00:24:57.443975\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"d934344e-3428-4e8e-976b-e634e348273f\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": {\"question\": \"What is the main topic of the document?\", \"context\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\\\\n\\\\nReferences\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\"}, \"config\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836606240592, \"init_bindings\": null}}}, \"kwargs\": {\"return_only_outputs\": false, \"include_run_info\": false}}, \"rets\": {\"question\": \"What is the main topic of the document?\", \"context\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\\\\n\\\\nReferences\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"text\": \"Attention-based neural machine translation\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:34.850224\", \"end_time\": \"2024-07-06T00:24:57.445980\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"21ea2b80-66b8-44d1-a5eb-31dc5aa0a533\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"__call__\"}}], \"args\": {\"inputs\": {\"question\": \"What is the main topic of the document?\", \"context\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\\\\n\\\\nReferences\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\"}, \"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836606240592, \"init_bindings\": null}}}, \"rets\": {\"question\": \"What is the main topic of the document?\", \"context\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\\\\n\\\\nReferences\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"text\": \"Attention-based neural machine translation\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:34.453136\", \"end_time\": \"2024-07-06T00:24:57.445980\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"a6c65702-ad04-4e18-a715-59c65b0c38ed\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}], \"args\": {\"inputs\": {\"input_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"What is the main topic of the document?\"}, \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForChainRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836606244432, \"init_bindings\": null}}}, \"rets\": {\"output_text\": \"Attention-based neural machine translation\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:34.069854\", \"end_time\": \"2024-07-06T00:24:57.447975\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"446f8505-de02-4a47-8cee-7e3cc3c89464\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": {\"input_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"What is the main topic of the document?\"}, \"config\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550653264, \"init_bindings\": null}}}, \"kwargs\": {\"return_only_outputs\": false, \"include_run_info\": false}}, \"rets\": {\"input_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"What is the main topic of the document?\", \"output_text\": \"Attention-based neural machine translation\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:33.696208\", \"end_time\": \"2024-07-06T00:24:57.450489\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"b6adb51c-b3bc-463a-bed9-edfcad2ccb9e\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}], \"args\": {\"inputs\": {\"input_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"What is the main topic of the document?\"}, \"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550653264, \"init_bindings\": null}}, \"tags\": null, \"metadata\": null}, \"rets\": {\"input_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"What is the main topic of the document?\", \"output_text\": \"Attention-based neural machine translation\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:33.325904\", \"end_time\": \"2024-07-06T00:24:57.453499\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"ebb5e318-fef4-4cf0-8206-a8232a61daef\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}], \"args\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550653264, \"init_bindings\": null}}, \"kwargs\": {\"input_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"What is the main topic of the document?\"}}, \"rets\": \"Attention-based neural machine translation\", \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:32.930556\", \"end_time\": \"2024-07-06T00:24:57.456498\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"44db583c-9eca-4fec-9e20-b68ac9baacc1\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}], \"args\": {\"inputs\": {\"query\": \"What is the main topic of the document?\"}, \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForChainRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836453472144, \"init_bindings\": null}}}, \"rets\": {\"result\": \"Attention-based neural machine translation\", \"source_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}]}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:31.326848\", \"end_time\": \"2024-07-06T00:24:57.457505\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"bf98d7d1-a574-4a82-b507-529e03cefd50\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": {\"query\": \"What is the main topic of the document?\"}, \"config\": {}, \"kwargs\": {\"return_only_outputs\": false, \"include_run_info\": false}}, \"rets\": {\"query\": \"What is the main topic of the document?\", \"result\": \"Attention-based neural machine translation\", \"source_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}]}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:30.968685\", \"end_time\": \"2024-07-06T00:24:57.458498\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"3b5cbc84-e20c-45a7-986e-e409a3a0ceb9\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}], \"args\": {\"inputs\": {\"query\": \"What is the main topic of the document?\"}}, \"rets\": {\"query\": \"What is the main topic of the document?\", \"result\": \"Attention-based neural machine translation\", \"source_documents\": [{\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\\\nOperations\\\\nSelf-Attention O(n2\\\\u00b7d) O(1) O(1)\\\\nRecurrent O(n\\\\u00b7d2) O(n) O(n)\\\\nConvolutional O(k\\\\u00b7n\\\\u00b7d2)O(1) O(logk(n))\\\\nSelf-Attention (restricted) O(r\\\\u00b7n\\\\u00b7d)O(1) O(n/r)\\\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\\\nlearned and \\\\ufb01xed [8].\\\\nIn this work, we use sine and cosine functions of different frequencies:\\\\nPE(pos,2i)=sin(pos/100002i/d model)\\\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\\\\u03c0to10000\\\\u00b72\\\\u03c0. We\\\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\\\nrelative positions, since for any \\\\ufb01xed offset k,PEpos+kcan be represented as a linear function of\\\\nPEpos.\\\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\\\nduring training.\\\\n4 Why Self-Attention\\\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi\\\\u2208Rd, such as a hidden\\\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\\\nconsider three desiderata.\\\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\\\nbe parallelized, as measured by the minimum number of sequential operations required.\\\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\\\nthe maximum path length between any two input and output positions in networks composed of the\\\\ndifferent layer types.\\\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\\\n6\", \"metadata\": {\"page\": 5, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}]}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:24:30.442769\", \"end_time\": \"2024-07-06T00:24:57.460004\"}, \"pid\": 2676, \"tid\": 3716}]}',\n",
       "        '{\"n_requests\": 0, \"n_successful_requests\": 0, \"n_classes\": 0, \"n_tokens\": 0, \"n_stream_chunks\": 0, \"n_prompt_tokens\": 0, \"n_completion_tokens\": 0, \"cost\": 0.0}',\n",
       "        '{\"start_time\": \"2024-07-06T00:24:30.442769\", \"end_time\": \"2024-07-06T00:24:57.460004\"}',\n",
       "        '2024-07-06T00:24:57.462020', 0.8, 0.9, 0.9,\n",
       "        list([{'args': {'prompt': 'What is the main topic of the document?', 'response': 'Attention-based neural machine translation'}, 'ret': 0.8, 'meta': {}}]),\n",
       "        list([{'args': {'source': ['[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6', 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'], 'statement': 'Attention-based neural machine translation'}, 'ret': 0.9, 'meta': {'reasons': 'STATEMENT 0:\\nCriteria: The Transformer model is able to train faster than other architectures because it uses self-attention mechanisms instead of recurrent layers.\\nThe Transformer model achieves better results than other architectures on translation tasks.\\nThe Transformer model is able to handle input and output modalities other than text.\\nThe Transformer model can be applied to tasks such as image, audio, and video generation.\\nThe Transformer model has a compatibility function that determines compatibility between attention keys.\\nThe Transformer model uses learned positional embeddings instead of sinusoidal positional encoding.\\nSupporting Evidence: In row (A) of Table 3, we observe that training time decreases as the attention key size increases. This suggests that the self-attention mechanism allows for faster training times.\\nScore: 8\\n\\nIn rows (B), (C), and (D) of Table 3, we observe that the Transformer model outperforms other architectures on various translation tasks.\\nScore: 10\\n\\nIn the last sentence of the paper, the authors mention their plans to extend the Transformer to problems involving input and output modalities other than text.\\nScore: 5 (as there is no information in the provided text that supports this statement)\\n\\nThere is no information in the provided text that supports this statement.\\nScore: 0 (as there is no information found)\\n\\nIn row (B) of Table 3, we observe that reducing the attention key size negatively affects the quality of the model. This suggests that there is a compatibility function in place to determine compatibility between attention keys.\\nScore: 8\\n\\nIn row (E) of Table 3, we observe that the use of learned positional embeddings instead of sinusoidal positional encoding leads to nearly identical results as the base model.\\nScore: 9 (as the score is close to 10)\\n\\nOverall, the Transformer model has several advantages over other architectures, including faster training times, better results on translation tasks, and the ability to handle input and output modalities other than text. However, there is no information in the provided text that supports the statement that the Transformer model can be applied to tasks such as image, audio, and video generation.\\n'}}]),\n",
       "        list([{'args': {'question': 'What is the main topic of the document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11'}, 'ret': 0.9, 'meta': {'reason': \"Criteria: Relevance of the given context to the question\\nSupporting Evidence: 1. The document is about attention-based neural machine translation, which is a relevant topic to the question.\\n2. The context provides information on different approaches to attention-based neural machine translation, including Minh-Thang Luong and Hieu Pham's work on effective approaches to attention-based neural machine translation, Ankur Parikh et al.'s work on a decomposable attention model, Romain Paulus et al.'s work on a deep reinforced model for abstractive summarization, Ofir Press and Lior Wolf's work on using the output embedding to improve language models, Rico Sennrich et al.'s work on neural machine translation of rare words with subword units, Noam Shazeer et al.'s work on outrageously large neural networks, Nitish Srivastava et al.'s work on dropout as a simple way to prevent neural networks from overfitting, Christian Szegedy et al.'s work on rethinking the inception architecture for computer vision, Yonghui Wu et al.'s work on Google's neural machine translation system, and Jie Zhou et al.'s work on deep recurrent models with fast-forward connections for neural machine translation.\\n3. The context provides relevant information to most parts of the question, including the different approaches to attention-based neural machine translation, the benefits of using output embeddings to improve language models, and the use of dropout as a regularization technique.\\n4. The score of 9 indicates that the given context is highly relevant to the question, providing valuable information on the topic of attention-based neural machine translation and its applications.\"}}, {'args': {'question': 'What is the main topic of the document?', 'context': 'Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2)O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d)O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i)=sin(pos/100002i/d model)\\nPE(pos,2i+1)=cos(pos/100002i/d model)\\nwhereposis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto10000·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k,PEpos+kcan be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [ 8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n)to another sequence of equal length (z1,...,z n), withxi,zi∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [ 11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlengthnis smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [ 25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the context to the question\\nSupporting Evidence: 1. The context provides relevant information about the topic of the document, which is aligned with the question asked. (1 point)\\n2. The context mentions specific layer types and their complexity per layer, sequential maximum path length, and positional encodings, which are all related to the question about the main topic of the document. (3 points)\\n3. The context explains why self-attention layers are faster than recurrent layers when the sequence length is smaller than the representation dimensionality d, which is a key factor in determining the main topic of the document. (2 points)\\n4. The context provides supporting evidence for the claim that self-attention could be restricted to considering only a neighborhood of size r to improve computational performance for tasks involving very long sequences, which is relevant to the question about the main topic of the document. (1 point)\\n\\nOverall, the context is highly relevant to the question and provides valuable information about the different layer types and their complexity, as well as the advantages and limitations of self-attention layers. Therefore, I scored it a 9 out of 10 for relevance.'}}, {'args': {'question': 'What is the main topic of the document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the given context to the question being asked.\\nSupporting Evidence: 1. The references provided are all related to the field of natural language processing and machine learning, which is the main topic of the document.\\n2. All of the references are relevant to the question being asked, as they provide more context on how to improve neural machine translation models.\\n3. The references provide a good mix of theoretical and practical works, including papers that propose new architectures and algorithms for neural machine translation, as well as papers that evaluate the effectiveness of these approaches.\\n4. The references are from reputable conferences and journals in the field, such as NIPS and ICLR, which further supports their relevance to the question being asked.\\n5. The references provide a good balance between short and long contexts, with some papers providing more detailed explanations of their methods and others providing simpler introductions to related work.\\n\\nOverall, the given context is highly relevant to the question being asked, as it provides a comprehensive overview of the current state of research in neural machine translation, including both theoretical and practical works.'}}, {'args': {'question': 'What is the main topic of the document?', 'context': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the context to the question\\nSupporting Evidence: 1. The context provides relevant information about the Transformer architecture, including variations on the model and their performance on different tasks. (3)\\n2. The context mentions the English-to-German translation development set and newstest2013, which are relevant to the question of the main topic of the document. (4)\\n3. The context provides a detailed explanation of the Transformer model and its components, including multi-headed self-attention, positional embedding, and dropout. This information is relevant to understanding the main topic of the document. (5)\\n4. The context mentions the advantages of attention-based models and the plan to apply them to other tasks, which are relevant to the question of the main topic of the document. (6)\\n\\nBased on these criteria, the context is highly relevant to the question of the main topic of the document, scoring a 9 out of 10.'}}]),\n",
       "        27, 0, 0.0],\n",
       "       ['RAG_QA_Chain_PDF',\n",
       "        '{\"tru_class_info\": {\"name\": \"TruChain\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.tru_chain\"}, \"bases\": [{\"name\": \"TruChain\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.tru_chain\"}, \"bases\": null}, {\"name\": \"App\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.app\"}, \"bases\": null}, {\"name\": \"AppDefinition\", \"module\": {\"package_name\": \"trulens_eval.schema\", \"module_name\": \"trulens_eval.schema.app\"}, \"bases\": null}, {\"name\": \"WithClassInfo\", \"module\": {\"package_name\": \"trulens_eval.utils\", \"module_name\": \"trulens_eval.utils.pyschema\"}, \"bases\": null}, {\"name\": \"SerialModel\", \"module\": {\"package_name\": \"trulens_eval.utils\", \"module_name\": \"trulens_eval.utils.serial\"}, \"bases\": null}, {\"name\": \"BaseModel\", \"module\": {\"package_name\": \"pydantic\", \"module_name\": \"pydantic.main\"}, \"bases\": null}, {\"name\": \"WithInstrumentCallbacks\", \"module\": {\"package_name\": \"trulens_eval\", \"module_name\": \"trulens_eval.instruments\"}, \"bases\": null}, {\"name\": \"Hashable\", \"module\": {\"package_name\": \"collections\", \"module_name\": \"collections.abc\"}, \"bases\": null}, {\"name\": \"Generic\", \"module\": {\"package_name\": \"\", \"module_name\": \"typing\"}, \"bases\": null}, {\"name\": \"object\", \"module\": {\"package_name\": \"\", \"module_name\": \"builtins\"}, \"bases\": null}]}, \"app_id\": \"RAG_QA_Chain_PDF\", \"tags\": \"-\", \"metadata\": {}, \"feedback_definitions\": [], \"feedback_mode\": \"with_app_thread\", \"root_class\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"app\": {\"name\": null, \"memory\": null, \"callbacks\": null, \"verbose\": false, \"tags\": null, \"metadata\": null, \"callback_manager\": null, \"combine_documents_chain\": {\"name\": null, \"memory\": null, \"callbacks\": null, \"verbose\": false, \"tags\": null, \"metadata\": null, \"callback_manager\": null, \"input_key\": \"input_documents\", \"output_key\": \"output_text\", \"llm_chain\": {\"name\": null, \"memory\": null, \"callbacks\": null, \"verbose\": false, \"tags\": null, \"metadata\": null, \"callback_manager\": null, \"prompt\": {\"name\": null, \"input_variables\": [\"context\", \"question\"], \"input_types\": {}, \"output_parser\": null, \"partial_variables\": {}, \"metadata\": null, \"tags\": null, \"template\": \"Use the following pieces of context to answer the question at the end. If you don\\'t know the answer, just say that you don\\'t know, don\\'t try to make up an answer.\\\\n\\\\n{context}\\\\n\\\\nQuestion: {question}\\\\nHelpful Answer:\", \"template_format\": \"f-string\", \"validate_template\": false}, \"llm\": {\"name\": null, \"cache\": null, \"verbose\": false, \"callbacks\": null, \"tags\": null, \"metadata\": null, \"custom_get_token_ids\": null, \"callback_manager\": null, \"pipeline\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"Text2TextGenerationPipeline\", \"module\": {\"package_name\": \"transformers.pipelines\", \"module_name\": \"transformers.pipelines.text2text_generation\"}, \"bases\": null}, \"id\": 2836523448528, \"init_bindings\": null}}, \"model_id\": \"gpt2\", \"model_kwargs\": null, \"pipeline_kwargs\": null, \"batch_size\": 4}, \"output_key\": \"text\", \"output_parser\": {\"name\": null}, \"return_final_only\": true, \"llm_kwargs\": {}}, \"document_prompt\": {\"name\": null, \"input_variables\": [\"page_content\"], \"input_types\": {}, \"output_parser\": null, \"partial_variables\": {}, \"metadata\": null, \"tags\": null, \"template\": \"{page_content}\", \"template_format\": \"f-string\", \"validate_template\": false}, \"document_variable_name\": \"context\", \"document_separator\": \"\\\\n\\\\n\"}, \"input_key\": \"query\", \"output_key\": \"result\", \"return_source_documents\": true, \"retriever\": {\"name\": null, \"tags\": [\"Chroma\", \"HuggingFaceEmbeddings\"], \"metadata\": null, \"vectorstore\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"Chroma\", \"module\": {\"package_name\": \"langchain_community.vectorstores\", \"module_name\": \"langchain_community.vectorstores.chroma\"}, \"bases\": null}, \"id\": 2836491291280, \"init_bindings\": null}}, \"search_type\": \"similarity\", \"search_kwargs\": {}}}, \"initial_app_loader_dump\": null, \"app_extra_json\": {}, \"selector_check_warning\": false, \"selector_nocheck\": false}',\n",
       "        'RetrievalQA(langchain.chains.retrieval_qa.base)',\n",
       "        'record_hash_50f4a5ce85a1092a30a6f84f893569bb',\n",
       "        '\"Who is the author of this document?\"',\n",
       "        '\"Nal Kalchbrenner and Stephan Gouws\"', '-',\n",
       "        '{\"record_id\": \"record_hash_50f4a5ce85a1092a30a6f84f893569bb\", \"app_id\": \"RAG_QA_Chain_PDF\", \"cost\": {\"n_requests\": 0, \"n_successful_requests\": 0, \"n_classes\": 0, \"n_tokens\": 0, \"n_stream_chunks\": 0, \"n_prompt_tokens\": 0, \"n_completion_tokens\": 0, \"cost\": 0.0}, \"perf\": {\"start_time\": \"2024-07-06T00:33:46.371196\", \"end_time\": \"2024-07-06T00:34:11.161489\"}, \"ts\": \"2024-07-06T00:34:11.162487\", \"tags\": \"-\", \"meta\": null, \"main_input\": \"Who is the author of this document?\", \"main_output\": \"Nal Kalchbrenner and Stephan Gouws\", \"main_error\": null, \"calls\": [{\"call_id\": \"2fa4a4b5-c7fe-4ca1-bf81-63511e4f7f02\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"get_relevant_documents\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"_get_relevant_documents\"}}], \"args\": {\"query\": \"Who is the author of this document?\", \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForRetrieverRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836669458960, \"init_bindings\": null}}}, \"rets\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:47.943327\", \"end_time\": \"2024-07-06T00:33:48.111945\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"0116609b-62d5-4685-b6e5-b7671264cc9c\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"get_relevant_documents\"}}], \"args\": {\"query\": \"Who is the author of this document?\", \"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550662416, \"init_bindings\": null}}, \"tags\": [], \"metadata\": {}, \"run_name\": null}, \"rets\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:47.626399\", \"end_time\": \"2024-07-06T00:33:48.113947\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"65382e55-3d9c-4c97-84d7-8b64493466fb\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.retriever\", \"method\": {\"obj\": {\"cls\": {\"name\": \"VectorStoreRetriever\", \"module\": {\"package_name\": \"langchain_core\", \"module_name\": \"langchain_core.vectorstores\"}, \"bases\": null}, \"id\": 2836485786704, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": \"Who is the author of this document?\", \"config\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550662416, \"init_bindings\": null}}}}, \"rets\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:47.307602\", \"end_time\": \"2024-07-06T00:33:48.114948\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"eba0f63d-1390-4c1f-8824-f32a93c203cc\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"_call\"}}], \"args\": {\"inputs\": {\"question\": \"Who is the author of this document?\", \"context\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\"}, \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForChainRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836669458896, \"init_bindings\": null}}}, \"rets\": {\"text\": \"Nal Kalchbrenner and Stephan Gouws\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:50.808967\", \"end_time\": \"2024-07-06T00:34:11.144931\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"6eb7af55-70cb-4ae0-80df-806798d5b420\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": {\"question\": \"Who is the author of this document?\", \"context\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\"}, \"config\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836606814736, \"init_bindings\": null}}}, \"kwargs\": {\"return_only_outputs\": false, \"include_run_info\": false}}, \"rets\": {\"question\": \"Who is the author of this document?\", \"context\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"text\": \"Nal Kalchbrenner and Stephan Gouws\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:50.474845\", \"end_time\": \"2024-07-06T00:34:11.146931\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"94208556-9570-4df9-acb0-a7eba411a900\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain.llm_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"LLMChain\", \"module\": {\"package_name\": \"langchain.chains\", \"module_name\": \"langchain.chains.llm\"}, \"bases\": null}, \"id\": 2836485791376, \"init_bindings\": null}, \"name\": \"__call__\"}}], \"args\": {\"inputs\": {\"question\": \"Who is the author of this document?\", \"context\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\"}, \"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836606814736, \"init_bindings\": null}}}, \"rets\": {\"question\": \"Who is the author of this document?\", \"context\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\\\\n\\\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\\\\n\\\\nAttention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\\\n\\\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"text\": \"Nal Kalchbrenner and Stephan Gouws\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:50.090571\", \"end_time\": \"2024-07-06T00:34:11.148020\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"f35ec913-06ab-4151-be3d-8aa4573e6bfc\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"_call\"}}], \"args\": {\"inputs\": {\"input_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"Who is the author of this document?\"}, \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForChainRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836606821968, \"init_bindings\": null}}}, \"rets\": {\"output_text\": \"Nal Kalchbrenner and Stephan Gouws\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:49.645794\", \"end_time\": \"2024-07-06T00:34:11.149972\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"6024ecf6-96f4-4ab3-bf5a-9c5cebaac0e0\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": {\"input_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"Who is the author of this document?\"}, \"config\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550662416, \"init_bindings\": null}}}, \"kwargs\": {\"return_only_outputs\": false, \"include_run_info\": false}}, \"rets\": {\"input_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"Who is the author of this document?\", \"output_text\": \"Nal Kalchbrenner and Stephan Gouws\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:49.194258\", \"end_time\": \"2024-07-06T00:34:11.151972\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"916c6911-1768-40c7-99fc-065917e37d0c\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"__call__\"}}], \"args\": {\"inputs\": {\"input_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"Who is the author of this document?\"}, \"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550662416, \"init_bindings\": null}}, \"tags\": null, \"metadata\": null}, \"rets\": {\"input_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"Who is the author of this document?\", \"output_text\": \"Nal Kalchbrenner and Stephan Gouws\"}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:48.802460\", \"end_time\": \"2024-07-06T00:34:11.154969\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"3dc47e82-51b4-4fa4-b549-3e14303a452f\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}, {\"path\": \"app.combine_documents_chain\", \"method\": {\"obj\": {\"cls\": {\"name\": \"StuffDocumentsChain\", \"module\": {\"package_name\": \"langchain.chains.combine_documents\", \"module_name\": \"langchain.chains.combine_documents.stuff\"}, \"bases\": null}, \"id\": 2836523446480, \"init_bindings\": null}, \"name\": \"run\"}}], \"args\": {\"callbacks\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManager\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836550662416, \"init_bindings\": null}}, \"kwargs\": {\"input_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}], \"question\": \"Who is the author of this document?\"}}, \"rets\": \"Nal Kalchbrenner and Stephan Gouws\", \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:48.460350\", \"end_time\": \"2024-07-06T00:34:11.158480\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"fae79f7b-58f0-4d9e-b190-d24311b0746a\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"_call\"}}], \"args\": {\"inputs\": {\"query\": \"Who is the author of this document?\"}, \"run_manager\": {\"__tru_non_serialized_object\": {\"cls\": {\"name\": \"CallbackManagerForChainRun\", \"module\": {\"package_name\": \"langchain_core.callbacks\", \"module_name\": \"langchain_core.callbacks.manager\"}, \"bases\": null}, \"id\": 2836669448464, \"init_bindings\": null}}}, \"rets\": {\"result\": \"Nal Kalchbrenner and Stephan Gouws\", \"source_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}]}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:46.997697\", \"end_time\": \"2024-07-06T00:34:11.158480\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"25d9e38f-42d7-4c85-b742-fee843c322c1\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}, {\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"invoke\"}}], \"args\": {\"input\": {\"query\": \"Who is the author of this document?\"}, \"config\": {}, \"kwargs\": {\"return_only_outputs\": false, \"include_run_info\": false}}, \"rets\": {\"query\": \"Who is the author of this document?\", \"result\": \"Nal Kalchbrenner and Stephan Gouws\", \"source_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}]}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:46.693422\", \"end_time\": \"2024-07-06T00:34:11.160487\"}, \"pid\": 2676, \"tid\": 3716}, {\"call_id\": \"d047ebc5-e049-4751-a26f-9580e68cdf74\", \"stack\": [{\"path\": \"app\", \"method\": {\"obj\": {\"cls\": {\"name\": \"RetrievalQA\", \"module\": {\"package_name\": \"langchain.chains.retrieval_qa\", \"module_name\": \"langchain.chains.retrieval_qa.base\"}, \"bases\": null}, \"id\": 2836511782864, \"init_bindings\": null}, \"name\": \"__call__\"}}], \"args\": {\"inputs\": {\"query\": \"Who is the author of this document?\"}}, \"rets\": {\"query\": \"Who is the author of this document?\", \"result\": \"Nal Kalchbrenner and Stephan Gouws\", \"source_documents\": [{\"page_content\": \"References\\\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\\\narXiv:1607.06450 , 2016.\\\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\\\nmachine translation. CoRR , abs/1406.1078, 2014.\\\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\\\npreprint arXiv:1610.02357 , 2016.\\\\n[7]Junyoung Chung, \\\\u00c7aglar G\\\\u00fcl\\\\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\\\narXiv:1308.0850 , 2013.\\\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\\\nRecognition , pages 770\\\\u2013778, 2016.\\\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\\\\u00fcrgen Schmidhuber. Gradient \\\\ufb02ow in\\\\nrecurrent nets: the dif\\\\ufb01culty of learning long-term dependencies, 2001.\\\\n[12] Sepp Hochreiter and J\\\\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\\\\n9(8):1735\\\\u20131780, 1997.\\\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\\\n[14] \\\\u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\\\non Learning Representations (ICLR) , 2016.\\\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\\\n2017.\\\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\\\nInInternational Conference on Learning Representations , 2017.\\\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\\\narXiv:1703.10722 , 2017.\\\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\\\narXiv:1703.03130 , 2017.\\\\n[20] Samy Bengio \\\\u0141ukasz Kaiser. Can active memory replace attention? In Advances in Neural\\\\nInformation Processing Systems, (NIPS) , 2016.\\\\n10\", \"metadata\": {\"page\": 9, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\\\n[22] Ankur Parikh, Oscar T\\\\u00e4ckstr\\\\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\\\n[24] O\\\\ufb01r Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\\\npreprint arXiv:1608.05859 , 2016.\\\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\\\nnov. Dropout: a simple way to prevent neural networks from over\\\\ufb01tting. Journal of Machine\\\\nLearning Research , 15(1):1929\\\\u20131958, 2014.\\\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\\\nAdvances in Neural Information Processing Systems 28 , pages 2440\\\\u20132448. Curran Associates,\\\\nInc., 2015.\\\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\\\\u20133112, 2014.\\\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\\\\u2019s neural machine\\\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\\\narXiv:1609.08144 , 2016.\\\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\\\n11\", \"metadata\": {\"page\": 10, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Attention Is All You Need\\\\nAshish Vaswani\\\\u2217\\\\nGoogle Brain\\\\navaswani@google.comNoam Shazeer\\\\u2217\\\\nGoogle Brain\\\\nnoam@google.comNiki Parmar\\\\u2217\\\\nGoogle Research\\\\nnikip@google.comJakob Uszkoreit\\\\u2217\\\\nGoogle Research\\\\nusz@google.com\\\\nLlion Jones\\\\u2217\\\\nGoogle Research\\\\nllion@google.comAidan N. Gomez\\\\u2217\\\\u2020\\\\nUniversity of Toronto\\\\naidan@cs.toronto.edu\\\\u0141ukasz Kaiser\\\\u2217\\\\nGoogle Brain\\\\nlukaszkaiser@google.com\\\\nIllia Polosukhin\\\\u2217\\\\u2021\\\\nillia.polosukhin@gmail.com\\\\nAbstract\\\\nThe dominant sequence transduction models are based on complex recurrent or\\\\nconvolutional neural networks that include an encoder and a decoder. The best\\\\nperforming models also connect the encoder and decoder through an attention\\\\nmechanism. We propose a new simple network architecture, the Transformer,\\\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\\\nentirely. Experiments on two machine translation tasks show these models to\\\\nbe superior in quality while being more parallelizable and requiring signi\\\\ufb01cantly\\\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\\\nto-German translation task, improving over the existing best results, including\\\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\\\nbest models from the literature.\\\\n1 Introduction\\\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\\\nin particular, have been \\\\ufb01rmly established as state of the art approaches in sequence modeling and\\\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\\\narchitectures [31, 21, 13].\\\\n\\\\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the \\\\ufb01rst Transformer models and\\\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\\\nattention and the parameter-free position representation and became the other person involved in nearly every\\\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\\\nef\\\\ufb01cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\\\nour research.\\\\n\\\\u2020Work performed while at Google Brain.\\\\n\\\\u2021Work performed while at Google Research.\\\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\", \"metadata\": {\"page\": 0, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}, {\"page_content\": \"Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\\\nper-word perplexities.\\\\nN d modeldffh d kdvPdrop\\\\u03f5lstrain PPL BLEU params\\\\nsteps (dev) (dev)\\\\u00d7106\\\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\\\n(A)1 512 512 5.29 24.9\\\\n4 128 128 5.00 25.5\\\\n16 32 32 4.91 25.8\\\\n32 16 16 5.01 25.4\\\\n(B)16 5.16 25.1 58\\\\n32 5.01 25.4 60\\\\n(C)2 6.11 23.7 36\\\\n4 5.19 25.3 50\\\\n8 4.88 25.5 80\\\\n256 32 32 5.75 24.5 28\\\\n1024 128 128 4.66 26.0 168\\\\n1024 5.12 25.4 53\\\\n4096 4.75 26.2 90\\\\n(D)0.0 5.77 24.6\\\\n0.2 4.95 25.5\\\\n0.0 4.67 25.3\\\\n0.2 5.47 25.7\\\\n(E) positional embedding instead of sinusoids 4.92 25.7\\\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\\\nfunction than dot product may be bene\\\\ufb01cial. We further observe in rows (C) and (D) that, as expected,\\\\nbigger models are better, and dropout is very helpful in avoiding over-\\\\ufb01tting. In row (E) we replace our\\\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\\\nresults to the base model.\\\\n7 Conclusion\\\\nIn this work, we presented the Transformer, the \\\\ufb01rst sequence transduction model based entirely on\\\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\\\nmulti-headed self-attention.\\\\nFor translation tasks, the Transformer can be trained signi\\\\ufb01cantly faster than architectures based\\\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\\\nmodel outperforms even all previously reported ensembles.\\\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\\\nto investigate local, restricted attention mechanisms to ef\\\\ufb01ciently handle large inputs and outputs\\\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\\\nThe code we used to train and evaluate our models is available at https://github.com/\\\\ntensorflow/tensor2tensor .\\\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\\\ncomments, corrections and inspiration.\\\\n9\", \"metadata\": {\"page\": 8, \"source\": \"D:/TruLens/attention.pdf\"}, \"type\": \"Document\"}]}, \"error\": null, \"perf\": {\"start_time\": \"2024-07-06T00:33:46.371196\", \"end_time\": \"2024-07-06T00:34:11.161489\"}, \"pid\": 2676, \"tid\": 3716}]}',\n",
       "        '{\"n_requests\": 0, \"n_successful_requests\": 0, \"n_classes\": 0, \"n_tokens\": 0, \"n_stream_chunks\": 0, \"n_prompt_tokens\": 0, \"n_completion_tokens\": 0, \"cost\": 0.0}',\n",
       "        '{\"start_time\": \"2024-07-06T00:33:46.371196\", \"end_time\": \"2024-07-06T00:34:11.161489\"}',\n",
       "        '2024-07-06T00:34:11.162487', 0.8, 0.7, 0.9,\n",
       "        list([{'args': {'prompt': 'Who is the author of this document?', 'response': 'Nal Kalchbrenner and Stephan Gouws'}, 'ret': 0.8, 'meta': {}}]),\n",
       "        list([{'args': {'source': ['References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10', '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11', 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.', 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'], 'statement': 'Nal Kalchbrenner and Stephan Gouws'}, 'ret': 0.7, 'meta': {'reasons': 'STATEMENT 0:\\nCriteria: \\nSupporting Evidence: In Table 3, we observe that reducing the attention key size dkhurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. (Location: Row B) Score: 8\\n\\nHypothesis 2: Nal Kalchbrenner and Stephan Gouws\\n\\nIn rows (C) and (D), we observe that bigger models are better, and dropout is very helpful in avoiding over-fitting. (Location: Rows C and D) Score: 9\\n\\nHypothesis 3: Nal Kalchbrenner and Stephan Gouws\\n\\nIn row (E), we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model. (Location: Row E) Score: 7\\n\\nNote that since there is no information overlap in the provided text, the scores are all 0-5.\\n'}}]),\n",
       "        list([{'args': {'question': 'Who is the author of this document?', 'context': 'References\\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450 , 2016.\\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733 , 2016.\\n[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR , abs/1406.1078, 2014.\\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357 , 2016.\\n[7]Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\\n[8]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\\n[9]Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850 , 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition , pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation ,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR) , 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nInInternational Conference on Learning Representations , 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722 , 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130 , 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS) , 2016.\\n10'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the given context to the question\\nSupporting Evidence: 1. The context provided is a list of references related to the field of natural language processing and machine learning, which is directly relevant to the question asked. (0)\\n2. The references are from reputable sources such as arXiv preprints, ICLR, NIPS, and ICML, which are well-known conferences and journals in the field. (1)\\n3. The context provides a good mix of different types of references, including research papers, conference proceedings, and book chapters, which covers different aspects of natural language processing and machine learning. (2)\\n4. The references are recent and cover the period from 2016 to 2017, which is relevant to the question asked as it pertains to recent advancements in the field. (3)\\n5. The context does not provide any irrelevant or redundant information, which makes it concise and easy to understand. (4)\\n\\nBased on these criteria, I score the relevance of the given context to the question as 9 out of 10.'}}, {'args': {'question': 'Who is the author of this document?', 'context': '[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing , 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859 , 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research , 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28 , pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems , pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144 , 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\\n11'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the given context to the question\\nSupporting Evidence: 1. The context provided is a list of papers and authors related to neural machine translation, which is directly relevant to the question asked.\\n2. All the papers listed are authored by researchers in the field of natural language processing, which further supports their relevance to the question.\\n3. The papers cover a range of topics related to attention-based neural machine translation, including effective approaches, decomposable attention models, and deep reinforced models for abstractive summarization. This breadth of coverage demonstrates the relevance of the context to different aspects of the question.\\n4. The authors listed are all well-known researchers in the field of natural language processing, which suggests that they have a deep understanding of the topic and can provide relevant insights to the question asked.\\n5. The context provides a good balance between short and long texts, with most papers having a length of around 10-20 pages. This indicates that the context is relevant to both short and longer questions.\\n\\nBased on these criteria, I score the relevance of the given context to the question as 9 out of 10.'}}, {'args': {'question': 'Who is the author of this document?', 'context': 'Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29,2,5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the context to the question\\nSupporting Evidence: 1. The context provided is a paper titled \"Attention Is All You Need\" by Ashish Vaswani et al., which is relevant to the question of machine translation and sequence transduction.\\n2. The paper discusses the Transformer architecture, which is the focus of the question.\\n3. The authors of the paper are mentioned in the context, which adds relevance to the question.\\n4. The abstract of the paper mentions the WMT 2014 English-to-German and English-to-French translation tasks, which are relevant to the question.\\n5. The paper provides a new simple network architecture based solely on attention mechanisms, which is relevant to the question of machine translation and sequence transduction.\\n6. The authors propose a new state-of-the-art BLEU score of 41.0 on the WMT 2014 English-to-French translation task, which is relevant to the question of machine translation quality.\\n7. The paper discusses the advantages of the Transformer architecture, such as parallelizability and reduced training time, which are relevant to the question of efficient machine translation models.\\n\\nBased on these criteria, the context provided is highly relevant to the question and scores a 9 out of 10.'}}, {'args': {'question': 'Who is the author of this document?', 'context': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d modeldffh d kdvPdropϵlstrain PPL BLEU params\\nsteps (dev) (dev)×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B)16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor .\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'}, 'ret': 0.9, 'meta': {'reason': 'Criteria: Relevance of the context to the question\\nSupporting Evidence: 1. The context provides relevant information about the Transformer architecture, including its components and performance on various tasks. (10)\\n2. The context mentions the author of the document, which is the person who wrote the paper about the Transformer model. (2)\\n3. The context does not provide any additional information that is directly related to the question. (4)\\n\\nBased on these criteria, I score the relevance of the context to the question as 9 out of 10. The context provides mostly relevant information about the Transformer architecture and its performance, but it also mentions the author of the document, which is not directly related to the question.'}}]),\n",
       "        24, 0, 0.0]], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_np = records.to_numpy()\n",
    "records_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Config file already exists. Skipping writing process.\n",
      "Credentials file already exists. Skipping writing process.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57dd3d608df54650ae791c44bbd4d9b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.1.3:8501 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Trulensvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
